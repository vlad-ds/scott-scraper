{
    "title": "Book Review: Superforecasting",
    "date": "February 4, 2016",
    "links": [
        "http://www.amazon.com/gp/product/1101905565/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1101905565&linkCode=as2&tag=slastacod-20&linkId=TG23EMZ3HULZD6XJ",
        "https://slatestarcodex.com/2016/01/02/2015-predictions-calibration-results/",
        "http://www.sjdm.org/dmidi/Cognitive_Reflection_Test.html",
        "https://www.gjopen.com/"
    ],
    "url": "https://slatestarcodex.com/2016/02/04/book-review-superforecasting/",
    "summary": "Key ideas:\n- Philip Tetlock's Expert Political Judgment experiment found that top pundits had no better accuracy than random chance, except for a few called \"superforecasters.\"\n- The Good Judgment Program was created to find the best forecasters in the world, and the top 2% were named \"superforecasters.\"\n- Superforecasters are not necessarily smarter or better-informed than others, but excel in their ability to understand logic and probability, and to challenge their own perceptions.\n- They use specific techniques for forecasting, such as numerical probability estimates, breaking problems down into smaller pieces, and actively fighting cognitive biases.\n\nKey learnings:\n- Superforecasters are not a specific phenomenon, but rather a result of understanding logic and probability and challenging one's own perceptions.\n- Numerical probability estimates, breaking problems down into pieces, and fighting cognitive biases are effective techniques for successful forecasting.\n\nKey questions:\n- What separates the top 2% of forecasters from the rest?\n- How can we distinguish between hedgehogs and foxes in terms of forecasting styles?\n- Can the techniques used by superforecasters be taught or learned by others?"
}