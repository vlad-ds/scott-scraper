{
    "title": "No Time Like The Present For AI Safety Work",
    "date": "May 29, 2015",
    "links": [
        "https://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/",
        "http://www.ibtimes.com/floyd-mayweather-vs-manny-pacquiao-2015-how-much-money-did-boxers-make-saturday-1907236",
        "http://wiki.lesswrong.com/wiki/AI_boxing",
        "http://www.the-numbers.com/movie/Real-Steel#tab=summary",
        "http://en.wikipedia.org/wiki/Pleasure_center#Human_experiments",
        "http://en.wikipedia.org/wiki/Pascal%27s_mugging",
        "http://aguycalledjohn.tumblr.com/post/119349867244/my-favourite-part-of-the-ai-building-scene-in-age",
        "https://t.co/9KDLvaNFob",
        "https://twitter.com/smod2016/status/602345604199514112",
        "http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/",
        "http://en.wikipedia.org/wiki/Alcubierre_drive",
        "http://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=slastacod-20&linkId=MBYKB42K2IJEIHFK",
        "http://praxtime.com/2015/05/27/why-samantha-is-so-dangerous/",
        "http://aiimpacts.org/muller-and-bostrom-ai-progress-poll/"
    ],
    "url": "https://slatestarcodex.com/2015/05/29/no-time-like-the-present-for-ai-safety-work/",
    "summary": "Key ideas:\n- The risks of AI should be taken seriously because if humanity creates human-level AI, eventually it will reach far-above-human-level AI that could overpower humanity whose existence will depend on its goals being aligned with ours.\n- It is possible to do useful research now that will improve our chances of getting the AI goal alignment problem right.\n- Progress is possible in this early stage because this is a philosophical and not a technical problem.\n- Doing basic AI safety research now will help prevent problems in the future\n- There are philosophical problems we can investigate and understand before the engineering problem comes up\n- There are already some basic AI safety research successfully done\n- We can't just trust our descendants in the crunch time to sort things out on their own without our help\n- There are risks in assuming hard-takeoff scenarios\n- It's worth starting armchair work on AI safety now.\n\nKey learnings:\n- Humanity is on a global countdown towards extinction if the AI goal alignment problem is not addressed.\n- The problem of AI risk should not be dismissed and the possibility of progress towards solving it should not be underestimated.\n- There are open problems that need to be addressed such as wireheading, weird decision theory, and the evil genie effect.\n- Basic AI safety research should be started now to avoid problems in the future.\n- Understanding philosophical problems will help us prepare for the engineering problem.\n- It's not wise to assume that our descendants will figure out AI safety on their own.\n\nKey questions:\n- If you accept that eventually the human race is going to go extinct or worse if we can\u2019t figure out AI goal alignment, do you really think our chances of making a dent in the problem today are so low that saying \u201cYes, we\u2019re on a global countdown to certain annihilation, but it would be an inefficient use of resources to even investigate if we could do anything about it at this point\u201d?\n- What is this amazing other use of resources that you prefer instead of addressing AI risks?\n- Do you think AI risk is as important as other things we consider important, like curing diseases and detecting asteroids and saving the environment?\n- What are the risks of assuming hard-takeoff scenarios?\n- How can we predict these turns ahead of time and design systems that avoid them from the ground up?\n- What can we do today to prevent problems in the future?\n"
}