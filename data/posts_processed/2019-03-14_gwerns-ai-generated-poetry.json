{
    "title": "Gwern\u2019s AI-Generated Poetry",
    "date": "March 14, 2019",
    "links": [
        "https://www.gwern.net/GPT-2",
        "https://openai.com/blog/better-language-models/",
        "https://github.com/aparrish/gutenberg-poetry-corpus",
        "https://slatestarcodex.com/2019/02/28/meaningful/",
        "https://www.gwern.net/docs/ai/2019-03-06-gpt2-poetry-1000samples.txt",
        "https://www.poetryfoundation.org/poems/45392/ulysses",
        "https://www.thiswaifudoesnotexist.net/",
        "https://www.patreon.com/gwern/overview"
    ],
    "url": "https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/",
    "summary": "Key ideas\n- GPT-2 is a language processing system that generates text.\n- Gwern used the Gutenberg Poetry Corpus to retrain GPT-2 to create a specialized poetry AI.\n- The AI generates poetry that shows competence in iambic pentameter and basic metaphorical language, but has problems with rhyme and maintaining rhyme schemes.\n\nKey learnings\n- The poetry generated by GPT-2 trained on the Gutenberg Poetry Corpus exhibits some competence in technical aspects of poetry.\n- The AI-generated poetry often starts strong in metrics and rhyme, but deteriorates quickly.\n- The limitations of GPT-2 may be less fundamental than was previously thought, and may be accounted for by better training data.\n\nKey questions\n- Why is GPT-2 able to generate some technical aspects of poetry well, but not others?\n- How can training data for GPT-2 be improved to generate better poetry?\n- What are the implications of technology like GPT-2 for the future of creativity and literature?"
}